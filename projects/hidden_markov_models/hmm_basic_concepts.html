<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hidden Markov Model (HMM)</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>

    <h1>Hidden Markov Model (HMM)</h1>

    <p class="indented">
    <ol>
        <li><a href="#section1">Markov Chain</a></li>
        <li><a href="#section2">Hidden Markov Model</a></li>
        <li><a href="#section3">Application of HMMs</a></li>
    </ol>
    </p>

    <h2>1. Markov Chain <a class="anchor" id="section1"></a></h2>
        <p>A Markov chain is a mathematical system that experiences <strong>transitions</strong> from one <strong>state</strong> to another according to certain <strong>probabilistic rules</strong>. A Markov Chain exhibits the Markov Property: the probability of future event actions are not dependent upon the steps that led up to the present state. The probabilities of transition from one state to another are specified by a <strong>transition matrix</strong>.</p>

    <h2>2. Hidden Markov Model (HMM) <a class="anchor" id="section2"></a></h2>
    <ul>
        <p>The HMM comprises of 2 main components: the hidden states and the observations generated by the hidden states.</p>
        <p>The HMM is the relationship between the hidden states and the observations using 2 sets of probabilities:</p>
        <ul>
            <li><strong>the transition probabilities</strong>: the probabilities of transitioning from 1 hidden state to another</li>
            <li><strong>the emission probabilities</strong>: the probabilities of an observation given a hidden state</li>
        </ul>
        <p>To train an HMM model, the parameters of the state transition probabilities and observation likelihoods are estimated using the Baum-Welch algorithm or the forward-backward algorithm. The parameters are iteratively updated until convergence.</p>
        <p>To decode the most likely sequence of the hidden states, the Viterbi algorithm is applied. This can be used to predict future observations or classify sequences.</p>
    </ul>

    <h2>3. Applications of HMMs <a class="anchor" id="section3"></a></h2>
    <ul>
        <li>handwriting recognition</li>
        <li>speech recognition</li>
        <li>object recognition in images and videos</li>
        <li>NLP: tagging morpho-syntaxique</li>
        <li>NLP: orthographic correction</li>
    </ul>

    <h2>References</h2>
    <ul>
        <li>[1] <a href="https://brilliant.org/wiki/markov-chains/">https://brilliant.org/wiki/markov-chains/</a></li>
        <li>[2] <a href="https://www.geeksforgeeks.org/hidden-markov-model-in-machine-learning/">https://www.geeksforgeeks.org/hidden-markov-model-in-machine-learning/</a></li>
    </ul>

</body>
</html>
