# Hidden Markov Model (HMM)

* [1. Markov Chain](#section1)
* [2. Hidden Markov Model](#section2)
* [2. Application of HMMs](#section3)

## 1. Markov Chain [1] <a class="anchor" id="section1"></a>
- A Markov chain is a mathematical system that experiences **transitions** from one **state** to another according to certain **probabilistic rules**.
- A Markov Chain exihibits the Markov Property: the probability of future event actions are not dependent upon the steps that led up to the present state.
- The probabilities of transition from one state to another are specified by a **transition matrix**.


|![MFLe5RY72V-small-markov-chain.png](attachment:MFLe5RY72V-small-markov-chain.png)
| :--:| 
| *example of markov chain [1]* |

## 2. Hidden Markov Model (HMM) [2] <a class="anchor" id="section2"></a>
- The HMM comprises of 2 main components: the hidden states and the observations generated by the hidden states.
- The HMM is the relationship between the hidden states and the observations using 2 sets of probabilities:
    1. **the transition probabilities**: the probabilities of transitioning from 1 hidden state to another
    2. **the emission probabilities**: the probabilities of an observation given a hidden state
- To train an HMM model, the parameters of the state transition probabilites and observation likelihoods are estimated using the Baume-Welch algorithm or the forward-backward algorithm. The parameters are iteratively updated until convergence.
- To decode the most likely sequence sequence of the hidden states, the Viterbi algorithm is applied. This can be used to predict future observations or classify sequences.

## 3. Applications of HMMs <a class="anchor" id="section3"></a>
- handwriting recognition
- speech recognition
- object recognition in images and videos
- NLP: tagging morpho-syntaxique
- NLP: orthograph correction

## References
[1] https://brilliant.org/wiki/markov-chains/

[2] https://www.geeksforgeeks.org/hidden-markov-model-in-machine-learning/

