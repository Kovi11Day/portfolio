<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HMM for Handwritten Digit Image Classification</title>
    <link rel="stylesheet" href="../../styles.css">

    <style>
        h2 {
            position: relative;
            display: block;
            font-size: 2rem;
            color: white;
            padding: 20px;
            text-align: center;
            z-index: 1;
            background-color: rgba(0, 0, 0, 0.5); /* Fallback for readability */
        }

        h2::before {
            content: "";
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
            opacity: 0.5; /* Set the opacity of the background image */
        }

        /* Different background images for each h2 */
        h2#project::before {
            background-image: url('../../photos/lmr.jpg');
            width: 100%;
        }

        table {
            width: 50%;
            margin: 20px auto;
            border-collapse: collapse; /* Ensures no double borders */
        }
        th, td {
            padding: 10px;
            border: 1px solid black; /* Adds border around cells */
            text-align: center;
        }
        th {
            background-color: #f4f4f4;
        }

    </style>
</head>
<body>
    <header>
        <h2 id="project">Location Mention Recognition using BERT</h2>
    </header>

    <section class="project">
        <div class="description">
            <h3>Project Description</h3>
            <p>This project involves developing an automated process for recognition of toponyms (place/ area/ street names) in microblogging posts. The aim is to help authorities in the face of disaster, to determine specific locations to concentrate resources such as medical aid, food and shelter.</p>
            <p>The microblogging data used will be Twitter (X) posts and a Location Mention Recognition system will be built.</p>
            <p>This work is based on this <a href="https://zindi.africa/competitions/microsoft-learn-location-mention-recognition-challenge">data challenge</a> where the labeled dataset has been provided.</p>
            </div> 

            <ul>
                <li><a href="https://github.com/Kovi11Day/portfolio/blob/main/projects/location_mention_recognition/LMR_portfolio.ipynb">The project code - Fine-tuning BERT</a></li>
                <li><a href="https://github.com/Kovi11Day/portfolio/blob/main/projects/location_mention_recognition/LMR_NER_portfolio.ipynb">The project code - BERT NER</a></li>
                <li><a href="https://github.com/Kovi11Day/portfolio/blob/main/projects/location_mention_recognition/LMR_portfolio_result_analysis.ipynb">The project code - Result Analysis</a></li>
            </ul>
        </div>
    </section>

    <section class="project">
        <div class="description">
            <h3>Data Description</h3>
            <p>The following are a few examples of the microblogging text and the location labels provided.</p>
            <table>
            <tr>
                <th>text</th>
                <th>location label</th>
            </tr>
            <tr>
                <td>RT @redcrossny: #RedCross in Ecuador on the ground helping following yesterdays earthquake there. Follow @cruzrojaecuador for updates.</td>
                <td>Ecuador</td>
            </tr>
            <tr>
                <td>Army National Guardsman missing after massive flooding hits Ellicott City, Maryland. Thoughts and prayers with Eddison Alexander Hermond and his family. I sure hope they find him alive and well.</td>
                <td>Ellicott City Maryland</td>
            </tr>
            <tr>
                <td>RT @DogNextDoorTO: PAY WHAT YOU CAN NAIL TRIMS All funds raised go to #FortMcMurray #wildfire rescue efforts Suggested donation amount $5 f</td>
                <td>FortMcMurray	</td>
            </tr>
        </table>
        </div>
    </section>

    <section class="project">
        <div class="description">
            <h3>Extracting Location</h3>
            <p>To answer to the problem of extracting the locations from the given text, 2 approaches will be tested.</p>
            <p>
                <ul>
                    <li>Use BERT pretrained Named-Entity Recognition to extract locations detected.</li>
                    <li>Fine-tuning BERT pretrained model on the given text corpus.</li>
                </ul>
            </p>
            <p>The metric used to evaluate the model will be the <strong>word error rate</strong>.</p>
        </div>
    </section>

    <section class="project">
        <div class="description">
            <h3>Results</h3>
            <table>
                <tr>
                    <th></th>
                    <th>BERT NER</th>
                    <th>BERT Fine-tuned</th>
                </tr>
                <tr>
                    <td>% missing values</td>
                    <td>15.949</td>
                    <td>0.253</td>
                </tr>
                <tr>
                    <td>word error rate</td>
                    <td>0.564</td>
                    <td>0.514</td>
                </tr>
            </table>

            <p>It is indeed odd to have a significantly higher percentage of missing values in the NER results while having an error rate close to that of the fine-tuned model.</p>

            <p>After further investigation, the results showed that the fine-tuned model contained repetitions for locations present several times in the text, while the labels only contains each location once. Removing these repetitions improved the word error rate of the fine-tuned model, as shown in the result table below.</p>

            <p>It is interesting to note that for most of the cases where the fine-tuned model was unable to make a prediction, the text did not contain any location and the given label was actually wrong. Some examples are shown below.</p>

            <table>
                <tr>
                    <th>text</th>
                    <th>label</th>
                </tr>
                <tr>
                    <td>Winds are strengthening as storm force gales and heavy rain expected in quake-affected areas. Updates: #eqnz</td>
                    <td>gales</td>
                </tr>
                <tr>
                    <td>You can help victims of Hurricane Irma and Hurricane Harvey by donating to the #RedCross.</td>
                    <td>Hurricane Harvey</td>
                </tr>
            </table>

            <p>It should be noted that the given labels may sometimes contain errors or are incomplete such as the following examples where the fine-tuned model may be more accurate.</p>
            <table>
                <tr>
                    <th>text</th>
                    <th>label</th>
                    <th>predicted location</th>
                </tr>
                <tr>
                    <td>RT @the_chill_son: I am active duty military, w/ supplies, trying to get to St. Croix. Currently flying to St. Thomas on Sunday. Help! #stc</td>
                    <td>St. Croix</td>
                    <td>st. croix. st. thomas.</td>
                </tr>
                <tr>
                    <td>DEVELOPING: Hurricane Matthew lashes Haiti & Cuba, amid States of Emergency along US east coast. @Miguelnbc reports now on @NBCNightlyNews.</td>
                    <td>Haiti Cuba</td>
                    <td>haiti cuba us</td>
                </tr>
                <tr>
                    <td>Whoa. Craven County (includes New Bern, isnt a county with beachfront areas) has ordered a mandatory evacuation #HurricaneFlorence</td>
                    <td>Whoa. Craven New Bern</td>
                    <td>craven county new bern</td>
                </tr>
            </table>

            <p></p>

            <table>
                <tr>
                    <th>Model</th>
                    <th>Word Error Rate</th>
                </tr>
                <tr>
                    <td>BERT NER</td>
                    <td>0.564</td>
                </tr>
                <tr>
                    <td>BERT Fine-tuned</td>
                    <td>0.514</td>
                </tr>
                <tr>
                    <td>BERT Fine-tuned no repetitions</td>
                    <td>0.412</td>
                </tr>
            </table>
            
            <p>Finally, another factor which penalises the score of the fine-tuned model is that the model extracts the locations in the same order of occurrence in the text while the lables may not respect that order. If the location labels respected the order of appearance in the text, the result of the fine-tuned model would be even better. Therefore, we can conclude that the fine-tuned model performs better than the BERT NER model.</p>

        </div>
    </section>
    
</body>
</html>
